# Docker GPU Configuration for Archon Reranking - Phase 1 Complete

**Project**: Archon Rag Debug  
**Task**: Phase 1 - Docker GPU Configuration for Reranking  
**Status**: ‚úÖ COMPLETED  
**Date**: 2025-09-11  
**Author**: AI IDE Agent

## üéØ Objective

Configure Docker Compose to enable NVIDIA GPU access for Qwen3 reranking models in the Archon system, addressing the critical performance bottleneck where Qwen3-4B was running on CPU only.

## üîß Implementation Details

### System Environment
- **GPU**: NVIDIA GeForce GTX 1080 Ti (11GB VRAM)
- **Docker**: Version 28.4.0
- **Docker Compose**: Version v2.39.2
- **Driver**: NVIDIA 570.172.08
- **CUDA**: Version 12.8
- **Runtime**: nvidia-container-runtime installed

### Docker Compose Configuration Changes

**File Modified**: `docker-compose.yml`

Added GPU configuration to the `archon-agents` service:

```yaml
# AI Agents Service (ML/Reranking)
archon-agents:
  # ... existing configuration ...
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  # ... rest of configuration ...
```

### Configuration Rationale

1. **Single GPU Allocation**: Used `count: 1` since system has only one GPU
2. **NVIDIA Driver**: Specified `driver: nvidia` for proper GPU detection
3. **GPU Capabilities**: Used `[gpu]` capability for general GPU access
4. **Service Target**: Applied to `archon-agents` service where reranking will be implemented

## ‚úÖ Verification Results

### GPU Access Test
```bash
$ docker exec Archon-Agents nvidia-smi
Thu Sep 11 19:06:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1080 Ti     Off |   00000000:01:00.0  On |                  N/A |
| 10%   56C    P0             63W /  280W |    1102MiB /  11264MiB |      4%      Default |
|                                         |                        |               MIG M. |
+-----------------------------------------+------------------------+----------------------+
```

### Memory Analysis
```bash
$ docker exec Archon-Agents python -c "
import subprocess
result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free', '--format=csv'], 
                      capture_output=True, text=True)
print(result.stdout)"

# Output:
# name, memory.total [MiB], memory.free [MiB]
# NVIDIA GeForce GTX 1080 Ti, 11264 MiB, 10063 MiB
```

### Service Health Check
```bash
$ docker compose ps
NAME            IMAGE                  COMMAND                   SERVICE         STATUS
Archon-Agents   archon-archon-agents   "/bin/sh -c 'sh -c \"‚Ä¶"   archon-agents   Up (healthy)
Archon-MCP      archon-archon-mcp      "python -m src.mcp.m‚Ä¶"    archon-mcp      Up (healthy)
Archon-Server   archon-archon-server   "/bin/sh -c 'sh -c \"‚Ä¶"   archon-server   Up (healthy)
Archon-UI       archon-frontend        "docker-entrypoint.s‚Ä¶"    frontend        Up (healthy)
```

## üìä Performance Implications

### Memory Planning for Reranking Models

| Model | Memory Requirement | GTX 1080 Ti Compatibility | Recommendation |
|-------|-------------------|---------------------------|----------------|
| **Qwen3-Reranker-0.6B** | ~2.4GB VRAM | ‚úÖ Excellent fit | **Recommended** |
| **Qwen3-Reranker-4B** | ~16GB VRAM | ‚ùå Exceeds capacity | Avoid |

### GPU Architecture Compatibility

- **GTX 1080 Ti**: Pascal architecture
- **Optimal Precision**: float32 (avoid float16/bfloat16 for compatibility)
- **Available VRAM**: 10GB+ free for model loading
- **Compute Capability**: Supports all required ML operations

## üîç Key Discoveries

### Current Service Architecture
- **Agents Service**: Lightweight container (~200MB) with minimal dependencies
- **No ML Libraries**: Current `requirements.agents.txt` excludes:
  - sentence-transformers
  - transformers
  - torch
  - accelerate

### Implementation Location
- **GPU Configuration**: Applied to agents service for future use
- **Actual ML Code**: Will need to be implemented in Server service where ML libraries exist
- **Alternative**: Add ML libraries to agents service in future phases

## üöÄ Next Phase Readiness

### Phase 2: Add Reranking Provider Credentials
- ‚úÖ GPU infrastructure ready
- ‚úÖ Container can access GPU hardware
- üîÑ Need credential service integration

### Phase 3: Refactor Reranking Strategy
- ‚úÖ Hardware acceleration available
- üîÑ Need HuggingFace transformers implementation
- üîÑ Need to remove CrossEncoder fallbacks

### Phase 4: UI Controls
- ‚úÖ Backend infrastructure prepared
- üîÑ Need provider selection interface

## üõ†Ô∏è Technical Implementation Notes

### Docker Compose Best Practices Applied
1. **Modern Syntax**: Used `deploy.resources.reservations.devices` (Docker Compose 2.30.0+)
2. **Explicit Configuration**: Specified driver, count, and capabilities
3. **Service Isolation**: Only added GPU access to service that needs it
4. **Resource Management**: Single GPU allocation prevents resource conflicts

### Alternative Configurations Considered
```yaml
# Alternative 1: Simple gpus attribute (requires Docker Compose 2.30.0+)
services:
  archon-agents:
    gpus: all

# Alternative 2: Specific GPU by ID
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          device_ids: ['0']
          capabilities: [gpu]
```

**Selected Approach**: `deploy.resources.reservations.devices` for maximum compatibility and explicit control.

## üìã Verification Checklist

- [x] nvidia-container-runtime installed on host
- [x] Docker Compose version supports GPU configuration
- [x] GPU accessible inside container via nvidia-smi
- [x] GPU memory information retrievable
- [x] All services start and remain healthy
- [x] No conflicts with existing service configuration
- [x] Container has proper GPU device access

## üéâ Success Metrics

1. **Hardware Access**: ‚úÖ GPU fully accessible inside container
2. **Memory Availability**: ‚úÖ 10GB+ VRAM free for model loading
3. **Service Stability**: ‚úÖ All services healthy after configuration
4. **Performance Ready**: ‚úÖ Infrastructure prepared for GPU-accelerated reranking

## üìù Future Considerations

### Performance Optimization
- Monitor VRAM usage during model loading
- Implement model caching strategy to keep loaded in VRAM
- Consider batch processing optimization for multiple queries

### Resource Management
- Set memory limits if multiple models are loaded
- Implement graceful degradation when GPU memory is exhausted
- Monitor GPU temperature and utilization

### Scaling Considerations
- Current configuration supports single GPU only
- For multi-GPU systems, consider device_ids specification
- Load balancing across multiple containers if needed

---

## üìö References

- [Docker GPU Support Documentation](https://docs.docker.com/compose/how-tos/gpu-support)
- [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-container-toolkit)
- [Docker Compose GPU Configuration](https://docs.docker.com/reference/compose-file/services/#gpu)

---

**Status**: Phase 1 complete - GPU infrastructure ready for reranking implementation  
**Next Task**: Phase 2 - Add Reranking Provider Credentials